<!DOCTYPE html>
<html>

<head>
    <meta charset="utf-8">
    <meta name="description" content="Robust Finetuning of Vision-Language-Action Robot Policies via Parameter Merging">
    <meta name="keywords" content="Vision-Language-Action Models, Robust Finetuning, Robot Learning, Model Merging">
    <meta name="viewport" content="width=device-width, initial-scale=1">
    <title>RETAIN</title>

    <link href="https://fonts.googleapis.com/css?family=Google+Sans|Noto+Sans|Castoro" rel="stylesheet">

    <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/bulma@0.9.4/css/bulma.min.css">
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/bulma-carousel@4.0.4/dist/css/bulma-carousel.min.css">
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/bulma-slider@2.0.5/dist/css/bulma-slider.min.css">
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.0.0/css/all.min.css">
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
    <link rel="stylesheet" href="../assets/css/styles.css">
    <!-- <link rel="stylesheet" href="./static/css/index.css"> -->
    <style>
        .video-success {
            border: 6px solid #90EE90;
            border-radius: 10px;
            box-sizing: border-box;
        }

        .video-failure {
            border: 6px solid #FF6B6B;
            border-radius: 10px;
            box-sizing: border-box;
        }
    </style>
    <link rel="icon" href="https://example.com/path/to/favicon.ico">
    <script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
    <script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
</head>


<body>

    <section class="hero">
        <div class="hero-body" style="padding-bottom: 0;">
            <div class="container is-max-desktop">
                <div class="columns is-centered">
                    <div class="column has-text-centered">
                        <h1 class="title is-2 publication-title">Robust Finetuning of Vision-Language-Action Robot
                            Policies via Parameter Merging</h1>
                        <!-- <h2 class="title" style="font-size: large; color: grey;"></h2> -->
                        <div class="is-size-5 publication-authors">
                            <span class="author-block">
                                <a href="https://yajatyadav.github.io/">Yajat Yadav</a><sup>*</sup>,
                            </span>
                            <span class="author-block">
                                <a href="https://zhouzypaul.github.io/">Zhiyuan Zhou</a><sup>*</sup>,
                            </span>
                            <span class="author-block">
                                <a href="https://wagenmaker.github.io">Andrew Wagenmaker</a><sup></sup>,
                            </span>
                            <span class="author-block">
                                <a href="https://kpertsch.github.io/">Karl Pertsch</a><sup></sup>,
                            </span>
                            <span class="author-block">
                                <a href="https://people.eecs.berkeley.edu/~svlevine/">Sergey Levine</a><sup></sup>
                            </span>
                        </div>

                        <div class="is-size-5 publication-authors">
                            <span class="author-block"><sup>
                                    <font size="-0.1">
                                </sup>UC Berkeley</font></span>
                            <span class="author-block"><sup>
                                    <font size="-0.1">*
                                </sup>Core Contributors </font></span>
                        </div>


                        <div class="column has-text-centered">
                            <div class="publication-links">
                                <!-- PDF Link. -->
                                <span class="link-block">
                                    <a href="https://arxiv.org/abs/2512.08333"
                                        class="external-link button is-normal is-rounded is-dark">
                                        <span class="icon">
                                            <i class="fas fa-file-pdf"></i>
                                        </span>
                                        <span>Paper</span>
                                    </a>
                                </span>
                            </div>

                        </div>
                    </div>
                </div>
            </div>
        </div>
    </section>


    <section class="hero teaser" style="padding-top: 0;">
        <div class="container is-max-desktop">
            <div class="hero-body">
                <img id="teaser" src="images/retain_teaser.png" alt="robust finetuning of VLA robot policies"
                    width="100%">
                <h2 class="subtitle has-text-centered">
                </h2>
            </div>
        </div>
    </section>

    <section class="section">
        <div class="container is-max-desktop">
            <div style="background-color: #f5f5f5; padding: 40px; border-radius: 10px; margin: 0 auto; max-width: 85%;">
                <h2 class="title is-2 has-text-centered">TL;DR</h2>
                <div class="content has-text-centered">
                    <p class="is-size-5">
                        <strong>Generalist robot policies overfit severely</strong> when finetuned on
                        limited demonstrations, losing their broad capabilities and failing to generalize to simple
                        variations of the target task. We introduce <strong>RETAIN (Robust finE-tuning wiTh pArameter
                            mergINg)</strong>,
                        which <strong>simply interpolates the weights</strong> of pretrained and finetuned models.
                        RETAIN achieves <strong>~40% higher OOD success rates</strong> on real robots while
                        <strong>retaining generalist abilities</strong> and enabling <strong>continual skill
                            acquisition</strong>.
                    </p>
                </div>
            </div>
        </div>
    </section>

    <section class="section">
        <div class="container is-max-desktop">
            <h2 class="title is-3">Robot Results</h2>
            <div class="content has-text-justified">
                <p>
                    RETAIN allows for robust, generalizable finetuning of vision-language-action (VLA) robot policies.
                    Below, we demonstrate
                    RETAIN's performance on two different robotic manipulation tasks, showing both in-distribution
                    success and out-of-distribution generalization compared to standard <strong>supervised finetuning
                        (SFT)</strong>.
                </p>
            </div>

            <!-- Plates Task Section -->
            <div style="margin-top: 40px; margin-bottom: 60px;">
                <h3 class="title is-4">Plates Task</h3>
                <div class="content has-text-justified" style="margin-bottom: 20px;">
                    <p>
                        In this task, the robot must precisely grab a plate and insert it into the grooves of a dish
                        rack. We first show successful SFT on the in-distribution environment, followed by
                        comparisons of SFT failures versus RETAIN successes on out-of-distribution evaluations.
                    </p>
                </div>

                <!-- Single success video -->
                <div style="width: 100%; text-align: center; margin-bottom: 10px;">
                    <span style="font-weight: bold; font-size: 1em;"> In-Distribution: SFT Success</span>
                </div>
                <div style="width: 100%; margin-bottom: 40px; display: flex; justify-content: center;">
                    <video controls autoplay loop muted playsinline class="video-success"
                        style="width: 60%; max-width: 600px;">
                        <source src="videos_sped_up/plates_id_2.mp4" type="video/mp4">
                        Your browser does not support the video tag.
                    </video>
                </div>

                <!-- OOD Comparison 1 -->
                <div style="width: 100%; text-align: center; margin-bottom: 10px;">
                    <span style="font-weight: bold; font-size: 1em;">Out-of-Distribution Scenario 1</span>
                </div>
                <div style="display: flex; justify-content: center; gap: 2%; margin-bottom: 30px;">
                    <div style="flex: 1; max-width: 45%; text-align: center;">
                        <div style="margin-bottom: 8px; font-weight: 600;">SFT Failure</div>
                        <video controls autoplay loop muted playsinline class="video-failure" style="width: 100%;">
                            <source src="videos_sped_up/plates_ood1_fail.mp4" type="video/mp4">
                            Your browser does not support the video tag.
                        </video>
                    </div>
                    <div style="flex: 1; max-width: 45%; text-align: center;">
                        <div style="margin-bottom: 8px; font-weight: 600;">RETAIN Success</div>
                        <video controls autoplay loop muted playsinline class="video-success" style="width: 100%;">
                            <source src="videos_sped_up/plates_ood1_success.mp4" type="video/mp4">
                            Your browser does not support the video tag.
                        </video>
                    </div>
                </div>

                <!-- OOD Comparison 2 -->
                <div style="width: 100%; text-align: center; margin-bottom: 10px;">
                    <span style="font-weight: bold; font-size: 1em;">Out-of-Distribution Scenario 2</span>
                </div>
                <div style="display: flex; justify-content: center; gap: 2%; margin-bottom: 30px;">
                    <div style="flex: 1; max-width: 45%; text-align: center;">
                        <div style="margin-bottom: 8px; font-weight: 600;">SFT Failure</div>
                        <video controls autoplay loop muted playsinline class="video-failure" style="width: 100%;">
                            <source src="videos_sped_up/plates_ood2_fail_bad.mp4" type="video/mp4">
                            Your browser does not support the video tag.
                        </video>
                    </div>
                    <div style="flex: 1; max-width: 45%; text-align: center;">
                        <div style="margin-bottom: 8px; font-weight: 600;">RETAIN Success</div>
                        <video controls autoplay loop muted playsinline class="video-success" style="width: 100%;">
                            <source src="videos_sped_up/plates_ood2_success.mp4" type="video/mp4">
                            Your browser does not support the video tag.
                        </video>
                    </div>
                </div>

                <!-- OOD Comparison 3 -->
                <div style="width: 100%; text-align: center; margin-bottom: 10px;">
                    <span style="font-weight: bold; font-size: 1em;">Out-of-Distribution Scenario 3</span>
                </div>
                <div style="display: flex; justify-content: center; gap: 2%; margin-bottom: 30px;">
                    <div style="flex: 1; max-width: 45%; text-align: center;">
                        <div style="margin-bottom: 8px; font-weight: 600;">SFT Failure</div>
                        <video controls autoplay loop muted playsinline class="video-failure" style="width: 100%;">
                            <source src="videos_sped_up/plates_ood3_fail_cropped.mp4" type="video/mp4">
                            Your browser does not support the video tag.
                        </video>
                    </div>
                    <div style="flex: 1; max-width: 45%; text-align: center;">
                        <div style="margin-bottom: 8px; font-weight: 600;">RETAIN Success</div>
                        <video controls autoplay loop muted playsinline class="video-success" style="width: 100%;">
                            <source src="videos_sped_up/plates_ood3_success_cropped.mp4" type="video/mp4">
                            Your browser does not support the video tag.
                        </video>
                    </div>
                </div>

            </div>

            <!-- Whiteboards Task Section -->
            <div style="margin-top: 40px; margin-bottom: 60px;">
                <h3 class="title is-4">Whiteboard Task</h3>
                <div class="content has-text-justified" style="margin-bottom: 20px;">
                    <p>
                        This task involves grabbing an eraser and wiping text off a whiteboard. Similar to the plates
                        task, we demonstrate in-distribution success followed
                        by out-of-distribution comparisons showing RETAIN's robustness.
                    </p>
                </div>

                <!-- Single success video -->
                <div style="width: 100%; text-align: center; margin-bottom: 10px;">
                    <span style="font-weight: bold; font-size: 1em;"> In-Distribution: SFT Success</span>
                </div>
                <div style="width: 100%; margin-bottom: 40px; display: flex; justify-content: center;">
                    <video controls autoplay loop muted playsinline class="video-success"
                        style="width: 60%; max-width: 600px;">
                        <source src="videos_sped_up/whiteboard_id.mp4" type="video/mp4">
                        Your browser does not support the video tag.
                    </video>
                </div>

                <!-- OOD Comparison 1 -->
                <div style="width: 100%; text-align: center; margin-bottom: 10px;">
                    <span style="font-weight: bold; font-size: 1em;">Out-of-Distribution Scenario 1</span>
                </div>
                <div style="display: flex; justify-content: center; gap: 2%; margin-bottom: 30px;">
                    <div style="flex: 1; max-width: 45%; text-align: center;">
                        <div style="margin-bottom: 8px; font-weight: 600;">SFT Failure</div>
                        <video controls autoplay loop muted playsinline class="video-failure" style="width: 100%;">
                            <source src="videos_sped_up/whiteboard_ood_corner_fail.mp4" type="video/mp4">
                            Your browser does not support the video tag.
                        </video>
                    </div>
                    <div style="flex: 1; max-width: 45%; text-align: center;">
                        <div style="margin-bottom: 8px; font-weight: 600;">RETAIN Success</div>
                        <video controls autoplay loop muted playsinline class="video-success" style="width: 100%;">
                            <source src="videos_sped_up/whiteboard_ood_corner_success.mp4" type="video/mp4">
                            Your browser does not support the video tag.
                        </video>
                    </div>
                </div>

                <!-- OOD Comparison 2 -->
                <div style="width: 100%; text-align: center; margin-bottom: 10px;">
                    <span style="font-weight: bold; font-size: 1em;">Out-of-Distribution Scenario 2</span>
                </div>
                <div style="display: flex; justify-content: center; gap: 2%; margin-bottom: 30px;">
                    <div style="flex: 1; max-width: 45%; text-align: center;">
                        <div style="margin-bottom: 8px; font-weight: 600;">SFT Failure</div>
                        <video controls autoplay loop muted playsinline class="video-failure" style="width: 100%;">
                            <source src="videos_sped_up/whiteboard_ood_table_fail.mp4" type="video/mp4">
                            Your browser does not support the video tag.
                        </video>
                    </div>
                    <div style="flex: 1; max-width: 45%; text-align: center;">
                        <div style="margin-bottom: 8px; font-weight: 600;">RETAIN Success</div>
                        <video controls autoplay loop muted playsinline class="video-success" style="width: 100%;">
                            <source src="videos_sped_up/whiteboard_ood_table_success.mp4" type="video/mp4">
                            Your browser does not support the video tag.
                        </video>
                    </div>
                </div>

                <!-- OOD Comparison 3 -->
                <div style="width: 100%; text-align: center; margin-bottom: 10px;">
                    <span style="font-weight: bold; font-size: 1em;">Out-of-Distribution Scenario 3</span>
                </div>
                <div style="display: flex; justify-content: center; gap: 2%; margin-bottom: 30px;">
                    <div style="flex: 1; max-width: 45%; text-align: center;">
                        <div style="margin-bottom: 8px; font-weight: 600;">SFT Failure</div>
                        <video controls autoplay loop muted playsinline class="video-failure" style="width: 100%;">
                            <source src="videos_sped_up/whiteboard_ood_room_fail.mp4" type="video/mp4">
                            Your browser does not support the video tag.
                        </video>
                    </div>
                    <div style="flex: 1; max-width: 45%; text-align: center;">
                        <div style="margin-bottom: 8px; font-weight: 600;">RETAIN Success</div>
                        <video controls autoplay loop muted playsinline class="video-success" style="width: 100%;">
                            <source src="videos_sped_up/whiteboard_ood_room_success.mp4" type="video/mp4">
                            Your browser does not support the video tag.
                        </video>
                    </div>
                </div>
            </div>
        </div>
    </section>

    <section class="section">
        <div class="container is-max-desktop ">
            <h2 class="title is-3">Motivation</h2>
            <div class="columns">
                <div class="column">
                    <div class="content has-text-justified">
                        <p class="is-size-5">
                            Recent <strong>generalist robot policies</strong> trained on large, diverse datasets have
                            demonstrated
                            remarkable capabilities across varied environments, objects, and tasks. However, they still
                            <strong>require adaptation</strong> for new downstream tasks or robot systems. While prior
                            work has
                            shown that finetuning with tens or hundreds of hours of data can yield robust policies,
                            existing finetuning approaches struggle in low-data (~100 demos) regimes: they
                            <strong>fail to preserve the pretrained model's generality</strong> and
                            <strong>cannot robustly generalize</strong> beyond the narrow conditions present in the
                            limited finetuning dataset. This creates a critical need for methods that can
                            <strong>leverage broad pretrained competencies to enable learning generalized skills from
                                narrow datasets</strong>.
                        </p>
                    </div>
                </div>
            </div>

            <!-- Why Current Approaches Don't Work -->
            <div style="margin-top: 40px;">
                <h3 class="title is-5">Why Current Finetuning Approaches Don't Work</h3>

                <!-- Overfitting to Environments -->
                <div>
                    <div class="custom-box" style="margin-bottom: 20px;">
                        <div class="content has-text-justified">
                            <p>
                                <strong>Overfitting across Environments:</strong> Standard SFT (training all parameters
                                of the model on the target task)
                                suffers from this overfitting acoss environments. The plot below highlights the large
                                gap between ID and OOD performance, as well as the gradual degradation of generalist
                                capability, as we finetune a VLA on 3 different LIBERO tasks. These issues persist
                                despite extensive tuning of hyperparameters like the learning rate schedule (see paper
                                for more details).
                            </p>
                        </div>
                    </div>
                    <div style="display: flex; justify-content: center; width: 90%; margin: 0 auto;">
                        <img src="images/overfitting_with_image.png" alt="Overfitting to specific environments"
                            style="width: 100%;">
                    </div>
                </div>
            </div>
        </div>
    </section>

    <section class="section">
        <div class="container is-max-desktop">
            <h2 class="title is-3">Method</h2>
            <div class="columns">
                <div class="column">
                    <div class="content has-text-justified">
                        <p>
                            RETAIN introduces a simple, efficient approach to robust finetuning through parameter
                            merging. The key insight is to merge the parameters of a pretrained Vision-Language-Action
                            (VLA)
                            model with those of a finetuned model using a weighted combination controlled by a merging
                            coefficient \(\alpha\).
                        </p>
                        <p> Through this linear interpolation in weight space, we are able to
                            combine the generalization capabilities of the pretrained model with the task-specific
                            adaptation of the finetuned model to robustly learn a more general version of the skill, and
                            thus generalize to OOD varaions of the target task while also retaining performance on tasks
                            from
                            pretraining.
                        </p>
                    </div>
                </div>
            </div>



            <!-- Mathematical Formulations -->
            <div class="columns">
                <div class="column">
                    <div class="content has-text-justified">

                        <p><strong>Model Merging </strong></p>
                        <p>$$\tilde{\theta} = (1 - \alpha) \cdot \theta_{pre} + \alpha \cdot \theta_{ft}$$</p>


                        <p>
                            where \(\theta_{pre}\) represents the pretrained model parameters,
                            \(\theta_{ft}\) represents the parameters after task-specific finetuning, and \(\alpha\) ∈
                            [0,
                            1] controls the balance between generalization and task specialization.
                        </p>

                        <p><strong>Finetuning Setting</strong></p>
                        <!-- <p>
                            For finetuning the model on the target task, we use the standard behavior cloning (BC)
                            objective:
                            $$L_{BC}(\theta; \mathcal{D}) = - \frac{1}{|\mathcal{D}|} \sum_{(s_t, a_t, T) \in
                            \mathcal{D}} \log \pi_\theta(a_t | s_t, T)$$
                            where \(\mathcal{D}\) is the finetuning dataset and \(\pi_\theta\) is the policy
                            parameterized by
                            \(\theta\).
                        </p> -->
                        <p>
                            We consider two finetuning settings: <strong>task-finetuning (task-FT)</strong>,
                            where we finetune the entire model on a target task dataset \(\mathcal{D}_{\tau}\), and
                            <strong>co-finetuning
                                (co-FT)</strong>, where we finetune on a mix of the target task dataset
                            \(\mathcal{D}_{\tau}\) and the
                            pretraining dataset \(\mathcal{D}_{pre}\).
                        </p>
                        <ul>
                            <li>Using model merging with task-FT, we call our method <strong>RETAIN-task-FT</strong>.
                            </li>
                            <li>Using model merging with co-FT, we call our method <strong>RETAIN-co-FT</strong>.</li>
                        </ul>




                    </div>
                </div>
            </div>
        </div>
    </section>

    <section class="section">
        <div class="container is-max-desktop">
            <h2 class="title is-3">Results</h2>

            <div class="content has-text-justified">
                <p>
                    RETAIN achieves <b>state-of-the-art performance</b> on robotic manipulation tasks when it comes to
                    maintaining strong generalization to out-of-distribution scenarios, as well as retaining generalist
                    capabilities on tasks from pretraining. Our results demonstrate that
                    parameter merging provides a simple yet effective approach to robust finetuning, significantly
                    outperforming standard task finetuning methods that suffer from overfitting. Across two DROID real
                    robot tasks and three LIBERO simulation tasks,
                    <b>RETAIN consistently achieves superior out-of-distribution performance</b> compared to
                    baseline methods.
                </p>
            </div>

            <div style="display: flex; justify-content: center; width: 100%; margin: 30px auto 30px auto;">
                <img src="images/avg_droid_results.png" alt="Main DROID results comparison" style="width: 100%;">
            </div>

            <!-- Generalist Evaluations Grid -->
            <div style="margin-top: 50px;">
                <h3 class="title is-4">Generalist Task Retention</h3>
                <div class="content has-text-justified" style="margin-bottom: 25px;">
                    <p>
                        Beyond excelling at OOD variations of the target tasks, RETAIN successfully preserces the
                        pretrained
                        model's general capabilities. Below is a sample of successful generalist evaluations performed
                        by RETAIN,
                        demonstrating its ability to execute diverse manipulation tasks from the original DROID
                        pretraining distribution.
                    </p>
                </div>

                <!-- 5x2 Video Grid -->
                <div style="display: grid; grid-template-columns: repeat(5, 1fr); gap: 10px; width: 100%;">
                    <video controls autoplay loop muted playsinline class="video-success" style="width: 100%;">
                        <source src="videos_sped_up/generalist/gen_1.mp4" type="video/mp4">
                    </video>
                    <video controls autoplay loop muted playsinline class="video-success" style="width: 100%;">
                        <source src="videos_sped_up/generalist/gen_2.mp4" type="video/mp4">
                    </video>
                    <video controls autoplay loop muted playsinline class="video-success" style="width: 100%;">
                        <source src="videos_sped_up/generalist/gen_3.mp4" type="video/mp4">
                    </video>
                    <video controls autoplay loop muted playsinline class="video-success" style="width: 100%;">
                        <source src="videos_sped_up/generalist/gen_4.mp4" type="video/mp4">
                    </video>
                    <video controls autoplay loop muted playsinline class="video-success" style="width: 100%;">
                        <source src="videos_sped_up/generalist/gen_5.mp4" type="video/mp4">
                    </video>
                    <video controls autoplay loop muted playsinline class="video-success" style="width: 100%;">
                        <source src="videos_sped_up/generalist/gen_6.mp4" type="video/mp4">
                    </video>
                    <video controls autoplay loop muted playsinline class="video-success" style="width: 100%;">
                        <source src="videos_sped_up/generalist/gen_7.mp4" type="video/mp4">
                    </video>
                    <video controls autoplay loop muted playsinline class="video-success" style="width: 100%;">
                        <source src="videos_sped_up/generalist/gen_8.mp4" type="video/mp4">
                    </video>
                    <video controls autoplay loop muted playsinline class="video-success" style="width: 100%;">
                        <source src="videos_sped_up/generalist/gen_9.mp4" type="video/mp4">
                    </video>
                    <video controls autoplay loop muted playsinline class="video-success" style="width: 100%;">
                        <source src="videos_sped_up/generalist/gen_10.mp4" type="video/mp4">
                    </video>
                </div>
            </div>
        </div>
    </section>

    <section class="section">
        <div class="container is-max-desktop ">
            <h2 class="title is-3">Takeaways </h2>
            <div class="columns">
                <div class="column">

                    <!-- Question 1: Does RETAIN scale with pretraining data? -->
                    <div style="background-color: #e8f4f8; padding: 30px; border-radius: 10px; margin-bottom: 25px;">
                        <div class="content has-text-justified">
                            <h3>RETAIN Scales with Pretraining Data</h3>
                            <p>
                                How well does RETAIN do when the pretraining model contains differing amounts of
                                generalist knowledge, as induced by different amounts of pretraining data? To study
                                this, we evaluate RETAIN-co-FT initialized with three different pretrained models: 1)
                                trained on a
                                subset of DROID, 2) trained on all of DROID, and 3) trained on all of DROID plus a large
                                dataset from Physical Intelligence (PI).
                            </p>
                            <p>
                                Our experiments demonstrate that RETAIN scales with
                                the amount of pretraining data, and can better “transfer” generalist knowledge from the
                                pretrained model to new scenes when the pretrained model is more general. The plots
                                below show the results averaged over the two tasks in the DROID evaluation.
                            </p>
                        </div>
                    </div>
                    <div
                        style="display: flex; justify-content: center; width: 90%; margin: 0 auto; margin-bottom: 60px;">
                        <img src="images/data_scaling.png" alt="RETAIN scaling with pretraining data"
                            style="width: 100%;">
                    </div>

                    <!-- Question 2: Can RETAIN be applied sequentially? -->
                    <div style="background-color: #f0f8e8; padding: 30px; border-radius: 10px; margin-bottom: 25px;">
                        <div class="content has-text-justified">
                            <h3>RETAIN can be applied sequentially</h3>

                            <p>
                                We extend the model merging above to enable continual task adaptation. This is done by
                                sequentially
                                merging the pretrained model with the finetuned model after each task.
                                $$
                                \tilde{\theta}_{n} = (1 - \alpha) \cdot \tilde{\theta}_{n-1} + \alpha \cdot \theta_{ft,
                                n}
                                $$
                                where \(\theta_{ft, n}\) represents the parameters
                                after finetuning the merged model \(\tilde{\theta}_{n-1}\) on the \(n\)-th task.
                            </p>
                            <!-- <img src="images/continual_merging.png" alt="Continual task adaptation"
                                style="width: 100%;"> -->

                            <p>
                                We sequentially apply RETAIN to learn the DROID plates and whiteboard tasks in sequence.
                                Compared to the strongest baseline of sequential co-finetuning on both tasks' datasets,
                                sequential RETAIN is able to maintain its robust generalization to out-of-distribution
                                scenarios for both tasks.
                            </p>
                        </div>
                    </div>
                    <div
                        style="display: flex; justify-content: center; width: 90%; margin: 0 auto; margin-bottom: 60px;">
                        <img src="images/droid_continual_learning.png" alt="Sequential application of RETAIN"
                            style="width: 100%;">
                    </div>

                    <!-- Question 3: What VLA modules matter most for merging? -->
                    <div style="background-color: #fef3e8; padding: 30px; border-radius: 10px; margin-bottom: 25px;">
                        <div class="content has-text-justified">
                            <h3>Language Parameters Matter Most for Merging</h3>
                            <p>
                                Vision-Language-Action models consist of a vision encoder (v), language encoder (l), and
                                often an action expert (a). To study the advantages of independtely merging these
                                different parameter groups, we extend the RETAIN merging formula to the following:
                            </p>

                            <p> $$
                                \begin{pmatrix}
                                \tilde{\theta}_v \\
                                \tilde{\theta}_l \\
                                \tilde{\theta}_a
                                \end{pmatrix}
                                = \left[1-\begin{pmatrix}
                                \alpha_v \\
                                \alpha_l \\
                                \alpha_a
                                \end{pmatrix}\right] \cdot \begin{pmatrix}
                                \theta_{pre,v} \\
                                \theta_{pre,l} \\
                                \theta_{pre,a}
                                \end{pmatrix} + \begin{pmatrix}
                                \alpha_v \\
                                \alpha_l \\
                                \alpha_a
                                \end{pmatrix} \cdot \begin{pmatrix}
                                \theta_{ft,v} \\
                                \theta_{ft,l} \\
                                \theta_{ft,a}
                                \end{pmatrix}
                                $$</p>

                            <p> After conducting a grid search over the coefficients \(\alpha_v\), \(\alpha_l\), and
                                \(\alpha_a\) in simulation, we find that the largest variation in OOD performance is
                                achieved when
                                \(\alpha_l\) is varied, as seen in the color gradient of the plot cube below.

                                We also find that the best performance for a given \(\alpha_l\) is achieved by setting
                                \(\alpha_v = \alpha_a = 1\), as seen in the second plot below.
                            </p>

                            <p> These results suggest that during model merging, it may suffice to only merge the
                                parameters of the language model backbone (\(\alpha_l < 1\)). To validate this
                                    hypothesis, we compare the OOD performance of merging all parameters with RETAIN to
                                    only merging language model parameters on 3 LIBERO tasks. The results are shown in
                                    the bar chart below.</p>
                        </div>
                    </div>

                    <!-- First row: 2 images -->
                    <div
                        style="display: flex; justify-content: center; gap: 2%; width: 90%; margin: 0 auto; margin-bottom: 20px;">
                        <img src="images/modality_wise_merging.png" style="width: 49%;" alt="Vision encoder merging">
                        <img src="images/modality_average_over_L_mugs.png" style="width: 49%;"
                            alt="Language encoder merging">
                    </div>
                    <!-- Second row: 1 image -->
                    <div
                        style="display: flex; justify-content: center; width: 90%; margin: 0 auto; margin-bottom: 60px;">
                        <img src="images/ood_multimodal_merging.png" style="width: 60%;" alt="Action decoder merging">
                    </div>

                    <!-- Question 4: How much does the alpha parameter matter?
                    <div style="background-color: #f4e8f8; padding: 30px; border-radius: 10px; margin-bottom: 25px;">
                        <div class="content has-text-justified">
                            <h3>How much does the alpha parameter matter?</h3>
                            <p>
                                The merging coefficient α controls the trade-off between retaining pretrained knowledge
                                and incorporating task-specific adaptations. We perform an ablation study varying
                                \(\alpha\) from
                                0 (pure finetuned model) to 1 (pure pretrained model) and evaluating on the 3 LIBERO
                                tasks. Lining up with real-world results, our results show that model merging helps
                                improve OOD performance as long as the merged model is not too deviated from the
                                finetuned model.
                            </p>
                            <p>
                                While we find in our real-world experiments that RETAIN is robust to different values of
                                \(\alpha\), slight tuning of it is still needed to achieve the best possible OOD
                                performance.
                                An exciting future dierction is determining a good heuristic for choosing the optimal
                                \(\alpha\) value for a given task and dataset.
                            </p>
                        </div>
                    </div>
                    <div
                        style="display: flex; justify-content: center; width: 90%; margin: 0 auto; margin-bottom: 60px;">
                        <img src="images/ood_type_and_alpha_ablation.png" alt="Alpha parameter sensitivity analysis"
                            style="width: 100%;">
                    </div> -->

                </div>
            </div>
        </div>
    </section>

    <section class="section" id="BibTeX">
        <div class="container is-max-desktop">
            <h2 class="title">BibTeX</h2>
            <pre>
      <code>@article{yadav2025retain,
        author = {Yajat Yadav and Zhiyuan Zhou and Andrew Wagenmaker and Karl Pertsch and Sergey Levine},
        title  = {Robust Finetuning of Vision-Language-Action Robot Policies via Parameter Merging},
        conference = {arXiv Pre-print},
        year = {2025},
        url = {https://arxiv.org/abs/2512.08333},
      }
      </code>
    </pre>
        </div>
    </section>
    <br><br>

    <footer class="footer">
        <div class="container is-max-desktop">
            <div class="columns is-centered">
                <div class="column is-8">
                    <div class="content">
                        <p>
                            Website template borrowed from <a
                                href="https://github.com/nerfies/nerfies.github.io">NeRFies</a>
                            under a
                            <a rel="license" href="http://creativecommons.org/licenses/by-sa/4.0/">Creative
                                Commons Attribution-ShareAlike 4.0 International License</a>.
                        </p>
                    </div>
                </div>
            </div>
        </div>
    </footer>

</body>

</html>