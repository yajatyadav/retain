<!DOCTYPE html>
<html>

<head>
    <meta charset="utf-8">
    <meta name="description" content="Robust Finetuning of Vision-Language-Action Robot Policies via Parameter Merging">
    <meta name="keywords" content="Vision-Language-Action Models, Robust Finetuning, Robot Learning, Model Merging">
    <meta name="viewport" content="width=device-width, initial-scale=1">
    <title>RETAIN</title>

    <link href="https://fonts.googleapis.com/css?family=Google+Sans|Noto+Sans|Castoro" rel="stylesheet">

    <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/bulma@0.9.4/css/bulma.min.css">
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/bulma-carousel@4.0.4/dist/css/bulma-carousel.min.css">
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/bulma-slider@2.0.5/dist/css/bulma-slider.min.css">
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.0.0/css/all.min.css">
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
    <link rel="stylesheet" href="../assets/css/styles.css">
    <!-- <link rel="stylesheet" href="./static/css/index.css"> -->
    <style>
        .video-success {
            border: 6px solid #90EE90;
            border-radius: 10px;
            box-sizing: border-box;
        }

        .video-failure {
            border: 6px solid #FF6B6B;
            border-radius: 10px;
            box-sizing: border-box;
        }
    </style>
    <link rel="icon" href="https://example.com/path/to/favicon.ico">
    <script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
    <script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
</head>


<body>

    <section class="hero">
        <div class="hero-body" style="padding-bottom: 0;">
            <div class="container is-max-desktop">
                <div class="columns is-centered">
                    <div class="column has-text-centered">
                        <h1 class="title is-2 publication-title">Robust Finetuning of Vision-Language-Action Robot
                            Policies via Parameter Merging</h1>
                        <!-- <h2 class="title" style="font-size: large; color: grey;"></h2> -->
                        <div class="is-size-5 publication-authors">
                            <span class="author-block">
                                <a href="https://yajatyadav.github.io/">Yajat Yadav</a><sup>*</sup>,
                            </span>
                            <span class="author-block">
                                <a href="https://zhouzypaul.github.io/">Zhiyuan Zhou</a><sup>*</sup>,
                            </span>
                            <span class="author-block">
                                <a href="https://wagenmaker.github.io">Andrew Wagenmaker</a><sup></sup>,
                            </span>
                            <span class="author-block">
                                <a href="https://kpertsch.github.io/">Karl Pertsch</a><sup></sup>,
                            </span>
                            <span class="author-block">
                                <a href="https://people.eecs.berkeley.edu/~svlevine/">Sergey Levine</a><sup></sup>
                            </span>
                        </div>

                        <div class="is-size-5 publication-authors">
                            <span class="author-block"><sup>
                                    <font size="-0.1">
                                </sup>UC Berkeley,</font></span>
                            <span class="author-block"><sup>
                                    <font size="-0.1">*
                                </sup>Core Contributors </font></span>
                        </div>


                        <div class="column has-text-centered">
                            <div class="publication-links">
                                <!-- PDF Link. -->
                                <span class="link-block">
                                    <a href="https://arxiv.org/abs/2512.08333"
                                        class="external-link button is-normal is-rounded is-dark">
                                        <span class="icon">
                                            <i class="fas fa-file-pdf"></i>
                                        </span>
                                        <span>Paper</span>
                                    </a>
                                </span>
                            </div>

                        </div>
                    </div>
                </div>
            </div>
        </div>
    </section>


    <section class="hero teaser" style="padding-top: 0;">
        <div class="container is-max-desktop">
            <div class="hero-body">
                <img id="teaser" src="images/retain_teaser.png" alt="robust finetuning of VLA robot policies"
                    width="100%">
                <h2 class="subtitle has-text-centered">
                </h2>
            </div>
        </div>
    </section>

    <section class="section">
        <div class="container is-max-desktop">
            <div style="background-color: #f5f5f5; padding: 40px; border-radius: 10px; margin: 0 auto; max-width: 85%;">
                <h2 class="title is-2 has-text-centered">TL;DR</h2>
                <div class="content has-text-centered">
                    <p class="is-size-5">
                        Generalist robot policies <strong>overfit severely</strong> when finetuned to a target task 
                        with limited demonstrations: not only do they fail to generalize to simple variations of the target
                        task, they lose their generalist capabilities as well.
                        We find that <strong>simply interpolating the weights of pretrained and finetuned models</strong>
                        can retain generalist abilities, and further, transfer the generalist knowledge to the target task to generalize to
                        more complex variations of it. We introduce a simple method based on this called <strong>RETAIN</strong>.
                    </p>
                </div>
            </div>
        </div>
    </section>

    <section class="section">
        <div class="container is-max-desktop">
            <h2 class="title is-3">RETAIN Enables Robust Finetuning</h2>
            <div class="content has-text-justified">
                <p>
                    <div style="background-color: #fef3e8; padding: 30px; border-radius: 10px; margin-bottom: 20px;">
                        In practice, when we finetune a policy, we don't simply want it to work only in the setting where
                        we collected the finetuning demonstration, but for it to complete the demonstrated task in a variety of contexts
                        or scenes. Therefore, <strong>we evaluate the performance of the finetuned policy in three settings</strong>:

                        <ul>
                            <li><strong>Target task in-distribution (ID):</strong> measures policy performance on the exact task observed in the finetuning dataset.</li>
                            <li><strong>Target task out-of-distribution (OOD):</strong> measures the performance on the target task, in scenarios not
                                observed in the finetuning dataset, such as changes in object instances, backgrounds, lighting conditions and
                                camera angles. This measures the robustness of the finetuned policy.</li>
                            <li>
                                <strong>Generalist tasks:</strong> measures policy performance on tasks other than the target task, but for which we would
                                expect the generalist policy to perform reasonably. This measures how well the finetuned policy retains
                                generalist capabilities from the pretrained model.
                            </li>
                        </ul>
                    </div>

                    We find that RETAIN achives robust and generalizable finetuning of vision-language-action (VLA) robot policies.
                    See two example tasks below on how RETAIN compares to SFT (supervised finetuning of all parameters on the target task) on the three evaluation settings.

                </p>
            </div>

            <!-- Plates Task Section -->
            <div style="margin-top: 40px; margin-bottom: 60px;">
                <h3 class="title is-4">Example Task 1: Placing Plates in a Dish Rack</h3>
                <div class="content has-text-justified" style="margin-bottom: 20px;">
                    <p>
                        In this task, the robot must precisely grab a plate and insert it vertically into the grooves of a dish
                        rack.
                    </p>
                </div>

                <!-- Single success video -->
                <div style="width: 100%; text-align: center; margin-bottom: 10px;">
                    <span style="font-weight: bold; font-size: 1em;"> In-Distribution: SFT succeeds</span>
                </div>
                <div style="width: 100%; margin-bottom: 40px; display: flex; justify-content: center;">
                    <video controls autoplay loop muted playsinline class="video-success"
                        style="width: 60%; max-width: 600px;">
                        <source src="videos_sped_up/plates_id_2.mp4" type="video/mp4">
                        Your browser does not support the video tag.
                    </video>
                </div>
                <div class="content has-text-justified" style="margin-bottom: 20px;">
                    <p>
                        In in-distribution evaluation (evaluation on the exact task observed in the finetuning dataset), normal SFT succeeds as expected.
                        However, <strong>when we evaluate the policy on out-of-distribution scenarios</strong> (some variations of the same semantic task, see examples below), <strong> SFT policies fail to generalize but 
                        RETAIN policies does</strong>.
                    </p>
                </div>

                <!-- OOD Comparison 1 -->
                <div style="width: 100%; text-align: center; margin-bottom: 10px;">
                    <span style="font-weight: bold; font-size: 1em;">Out-of-Distribution Scenario 1</span>
                </div>
                <div style="display: flex; justify-content: center; gap: 2%; margin-bottom: 30px;">
                    <div style="flex: 1; max-width: 45%; text-align: center;">
                        <div style="margin-bottom: 8px; font-weight: 600;">SFT fails</div>
                        <video controls autoplay loop muted playsinline class="video-failure" style="width: 100%;">
                            <source src="videos_sped_up/plates_ood1_fail.mp4" type="video/mp4">
                            Your browser does not support the video tag.
                        </video>
                    </div>
                    <div style="flex: 1; max-width: 45%; text-align: center;">
                        <div style="margin-bottom: 8px; font-weight: 600;">RETAIN succeeds</div>
                        <video controls autoplay loop muted playsinline class="video-success" style="width: 100%;">
                            <source src="videos_sped_up/plates_ood1_success.mp4" type="video/mp4">
                            Your browser does not support the video tag.
                        </video>
                    </div>
                </div>

                <!-- OOD Comparison 2 -->
                <div style="width: 100%; text-align: center; margin-bottom: 10px;">
                    <span style="font-weight: bold; font-size: 1em;">Out-of-Distribution Scenario 2</span>
                </div>
                <div style="display: flex; justify-content: center; gap: 2%; margin-bottom: 30px;">
                    <div style="flex: 1; max-width: 45%; text-align: center;">
                        <div style="margin-bottom: 8px; font-weight: 600;">SFT fails</div>
                        <video controls autoplay loop muted playsinline class="video-failure" style="width: 100%;">
                            <source src="videos_sped_up/plates_ood2_fail_bad.mp4" type="video/mp4">
                            Your browser does not support the video tag.
                        </video>
                    </div>
                    <div style="flex: 1; max-width: 45%; text-align: center;">
                        <div style="margin-bottom: 8px; font-weight: 600;">RETAIN succeeds</div>
                        <video controls autoplay loop muted playsinline class="video-success" style="width: 100%;">
                            <source src="videos_sped_up/plates_ood2_success.mp4" type="video/mp4">
                            Your browser does not support the video tag.
                        </video>
                    </div>
                </div>

                <!-- OOD Comparison 3 -->
                <div style="width: 100%; text-align: center; margin-bottom: 10px;">
                    <span style="font-weight: bold; font-size: 1em;">Out-of-Distribution Scenario 3</span>
                </div>
                <div style="display: flex; justify-content: center; gap: 2%; margin-bottom: 30px;">
                    <div style="flex: 1; max-width: 45%; text-align: center;">
                        <div style="margin-bottom: 8px; font-weight: 600;">SFT fails</div>
                        <video controls autoplay loop muted playsinline class="video-failure" style="width: 100%;">
                            <source src="videos_sped_up/plates_ood3_fail_cropped.mp4" type="video/mp4">
                            Your browser does not support the video tag.
                        </video>
                    </div>
                    <div style="flex: 1; max-width: 45%; text-align: center;">
                        <div style="margin-bottom: 8px; font-weight: 600;">RETAIN succeeds</div>
                        <video controls autoplay loop muted playsinline class="video-success" style="width: 100%;">
                            <source src="videos_sped_up/plates_ood3_success_cropped.mp4" type="video/mp4">
                            Your browser does not support the video tag.
                        </video>
                    </div>
                </div>

            </div>

            <!-- Whiteboards Task Section -->
            <div style="margin-top: 40px; margin-bottom: 60px;">
                <h3 class="title is-4">Example Task 2: Wiping Whiteboard</h3>
                <div class="content has-text-justified" style="margin-bottom: 20px;">
                    <p>
                        This task involves grabbing an eraser and wiping text off a whiteboard. Similarly,
                        we find that SFT policies also fail to generalize to OOD variations of the task, but RETAIN policies succeed.
                    </p>
                </div>

                <!-- Single success video -->
                <div style="width: 100%; text-align: center; margin-bottom: 10px;">
                    <span style="font-weight: bold; font-size: 1em;"> In-Distribution: SFT succeeds</span>
                </div>
                <div style="width: 100%; margin-bottom: 40px; display: flex; justify-content: center;">
                    <video controls autoplay loop muted playsinline class="video-success"
                        style="width: 60%; max-width: 600px;">
                        <source src="videos_sped_up/whiteboard_id.mp4" type="video/mp4">
                        Your browser does not support the video tag.
                    </video>
                </div>

                <!-- OOD Comparison 1 -->
                <div style="width: 100%; text-align: center; margin-bottom: 10px;">
                    <span style="font-weight: bold; font-size: 1em;">Out-of-Distribution Scenario 1</span>
                </div>
                <div style="display: flex; justify-content: center; gap: 2%; margin-bottom: 30px;">
                    <div style="flex: 1; max-width: 45%; text-align: center;">
                        <div style="margin-bottom: 8px; font-weight: 600;">SFT fails</div>
                        <video controls autoplay loop muted playsinline class="video-failure" style="width: 100%;">
                            <source src="videos_sped_up/whiteboard_ood_corner_fail.mp4" type="video/mp4">
                            Your browser does not support the video tag.
                        </video>
                    </div>
                    <div style="flex: 1; max-width: 45%; text-align: center;">
                        <div style="margin-bottom: 8px; font-weight: 600;">RETAIN succeeds</div>
                        <video controls autoplay loop muted playsinline class="video-success" style="width: 100%;">
                            <source src="videos_sped_up/whiteboard_ood_corner_success.mp4" type="video/mp4">
                            Your browser does not support the video tag.
                        </video>
                    </div>
                </div>

                <!-- OOD Comparison 2 -->
                <div style="width: 100%; text-align: center; margin-bottom: 10px;">
                    <span style="font-weight: bold; font-size: 1em;">Out-of-Distribution Scenario 2</span>
                </div>
                <div style="display: flex; justify-content: center; gap: 2%; margin-bottom: 30px;">
                    <div style="flex: 1; max-width: 45%; text-align: center;">
                        <div style="margin-bottom: 8px; font-weight: 600;">SFT fails</div>
                        <video controls autoplay loop muted playsinline class="video-failure" style="width: 100%;">
                            <source src="videos_sped_up/whiteboard_ood_table_fail.mp4" type="video/mp4">
                            Your browser does not support the video tag.
                        </video>
                    </div>
                    <div style="flex: 1; max-width: 45%; text-align: center;">
                        <div style="margin-bottom: 8px; font-weight: 600;">RETAIN succeeds</div>
                        <video controls autoplay loop muted playsinline class="video-success" style="width: 100%;">
                            <source src="videos_sped_up/whiteboard_ood_table_success.mp4" type="video/mp4">
                            Your browser does not support the video tag.
                        </video>
                    </div>
                </div>

                <!-- OOD Comparison 3 -->
                <div style="width: 100%; text-align: center; margin-bottom: 10px;">
                    <span style="font-weight: bold; font-size: 1em;">Out-of-Distribution Scenario 3</span>
                </div>
                <div style="display: flex; justify-content: center; gap: 2%; margin-bottom: 30px;">
                    <div style="flex: 1; max-width: 45%; text-align: center;">
                        <div style="margin-bottom: 8px; font-weight: 600;">SFT fails</div>
                        <video controls autoplay loop muted playsinline class="video-failure" style="width: 100%;">
                            <source src="videos_sped_up/whiteboard_ood_room_fail.mp4" type="video/mp4">
                            Your browser does not support the video tag.
                        </video>
                    </div>
                    <div style="flex: 1; max-width: 45%; text-align: center;">
                        <div style="margin-bottom: 8px; font-weight: 600;">RETAIN succeeds</div>
                        <video controls autoplay loop muted playsinline class="video-success" style="width: 100%;">
                            <source src="videos_sped_up/whiteboard_ood_room_success.mp4" type="video/mp4">
                            Your browser does not support the video tag.
                        </video>
                    </div>
                </div>
            </div>

            <!-- Generalist Evaluations Grid -->
            <div style="margin-top: 50px;">
                <h3 class="title is-4">RETAIN Preserves Generalist Capabilities</h3>
                <div class="content has-text-justified" style="margin-bottom: 25px;">
                    <p>
                        In addition to achieving robust generalization to OOD variations of the target task, RETAIN also preserves the generalist capabilities of the pretrained model.
                        In the following generalist evaluations, we evaluate the RETAIN policy on tasks from the pretraining distribution.
                    </p>
                </div>

                <!-- 5x2 Video Grid -->
                <div style="display: grid; grid-template-columns: repeat(5, 1fr); gap: 10px; width: 100%;">
                    <div style="text-align: center;">
                        <video controls autoplay loop muted playsinline class="video-success" style="width: 100%;">
                            <source src="videos_sped_up/generalist/gen_1.mp4" type="video/mp4">
                        </video>
                        <div style="margin-top: 8px;">put the spoon in the dishrack</div>
                    </div>
                    <div style="text-align: center;">
                        <video controls autoplay loop muted playsinline class="video-success" style="width: 100%;">
                            <source src="videos_sped_up/generalist/gen_2.mp4" type="video/mp4">
                        </video>
                        <div style="margin-top: 8px;">put the marker in the cup</div>
                    </div>
                    <div style="text-align: center;">
                        <video controls autoplay loop muted playsinline class="video-success" style="width: 100%;">
                            <source src="videos_sped_up/generalist/gen_3.mp4" type="video/mp4">
                        </video>
                        <div style="margin-top: 8px;">wipe the table</div>
                    </div>
                    <div style="text-align: center;">
                        <video controls autoplay loop muted playsinline class="video-success" style="width: 100%;">
                            <source src="videos_sped_up/generalist/gen_4.mp4" type="video/mp4">
                        </video>
                        <div style="margin-top: 8px;">close the drawer</div>
                    </div>
                    <div style="text-align: center;">
                        <video controls autoplay loop muted playsinline class="video-success" style="width: 100%;">
                            <source src="videos_sped_up/generalist/gen_5.mp4" type="video/mp4">
                        </video>
                        <div style="margin-top: 8px;">put the tape in the purple bowl</div>
                    </div>
                    <div style="text-align: center;">
                        <video controls autoplay loop muted playsinline class="video-success" style="width: 100%;">
                            <source src="videos_sped_up/generalist/gen_6.mp4" type="video/mp4">
                        </video>
                        <div style="margin-top: 8px;">put the plate on the table</div>
                    </div>
                    <div style="text-align: center;">
                        <video controls autoplay loop muted playsinline class="video-success" style="width: 100%;">
                            <source src="videos_sped_up/generalist/gen_7.mp4" type="video/mp4">
                        </video>
                        <div style="margin-top: 8px;">put the black sponge in the blue bowl</div>
                    </div>
                    <div style="text-align: center;">
                        <video controls autoplay loop muted playsinline class="video-success" style="width: 100%;">
                            <source src="videos_sped_up/generalist/gen_8.mp4" type="video/mp4">
                        </video>
                        <div style="margin-top: 8px;">put the stapler on the notebook</div>
                    </div>
                    <div style="text-align: center;">
                        <video controls autoplay loop muted playsinline class="video-success" style="width: 100%;">
                            <source src="videos_sped_up/generalist/gen_9.mp4" type="video/mp4">
                        </video>
                        <div style="margin-top: 8px;">put the watermelon in the purple bowl</div>
                    </div>
                    <div style="text-align: center;">
                        <video controls autoplay loop muted playsinline class="video-success" style="width: 100%;">
                            <source src="videos_sped_up/generalist/gen_10.mp4" type="video/mp4">
                        </video>
                        <div style="margin-top: 8px;">put the red bottle in the black bowl</div>
                    </div>
                </div>
            </div>
        </div>

        </div>
    </section>


    </section>

    <section class="section">
        <div class="container is-max-desktop ">
            <h2 class="title is-3">Motivation</h2>
            <div class="columns">
                <div class="column">
                    <div class="content has-text-justified">
                        <p class="is-size-5">
                            Generalist robot policies have demonstrated strong
                            capabilities across varied environments, objects, and tasks. However, they still
                            <strong>require adaptation</strong> for new downstream tasks or robot systems. 
                            Naive finetuning approaches suffer from <strong>overfitting</strong> and
                            <strong>fail to preserve the pretrained model's generality</strong> and
                            <strong>cannot robustly generalize</strong> beyond the narrow conditions present in the
                            limited finetuning dataset. This creates a critical need for methods that can
                            <strong>leverage broad pretrained competencies to enable learning generalized skills</strong>.
                        </p>
                    </div>
                </div>
            </div>

            <!-- Why Current Approaches Don't Work -->
            <div style="margin-top: 40px;">
                <h3 class="title is-5">Naive Finetuning Suffers from Overfitting</h3>

                <!-- Overfitting to Environments -->
                <div>
                    <div class="custom-box" style="margin-bottom: 20px;">
                        <div class="content has-text-justified">
                            <p>
                                As an example, we finetune a VLA on 3 different LIBERO tasks with standard SFT finetuning, training
                                all parameters of the model on the target task.
                                While the ID performance improves with gradient updates, the Generalist performance drastically degrades 
                                as we finetune for longer. This shows that naive finetuning has overfitted the model to the exact dataset distribution,
                                and suffers from catastrophic forgetting of pretraining abilities.
                                Moreover, there is a large gap between ID and OOD performance, suggesting the model has failed to generalize 
                                to small variations of the target task because of overfitting.
                            </p>
                        </div>
                    </div>
                    <div style="display: flex; justify-content: center; width: 90%; margin: 0 auto;">
                        <img src="images/overfitting_with_image.png" alt="Overfitting to specific environments"
                            style="width: 100%;">
                    </div>
                </div>
            </div>
        </div>
    </section>

    <section class="section">
        <div class="container is-max-desktop">
            <h2 class="title is-3">A Simple Solution: RETAIN</h2>
            <!-- <div class="columns">
                <div class="column">
                    <div class="content has-text-justified">
                        <p>
                            RETAIN introduces a simple, efficient approach to robust finetuning through parameter
                            merging. The key insight is to merge the parameters of a pretrained Vision-Language-Action
                            (VLA)
                            model with those of a finetuned model using a weighted combination controlled by a merging
                            coefficient \(\alpha\).
                        </p>
                        <p> Through this linear interpolation in weight space, we are able to
                            combine the generalization capabilities of the pretrained model with the task-specific
                            adaptation of the finetuned model to robustly learn a more general version of the skill, and
                            thus generalize to OOD varaions of the target task while also retaining performance on tasks
                            from
                            pretraining.
                        </p>
                    </div>
                </div>
            </div> -->



            <!-- Mathematical Formulations -->
            <div class="columns">
                <div class="column">
                    <div class="content has-text-justified">

                        <p><strong>Model Merging </strong></p>
                        <p>$$\tilde{\theta} = (1 - \alpha) \cdot \theta_{pre} + \alpha \cdot \theta_{ft}$$</p>


                        <p>
                            We can combine the pretrained model \(\theta_{pre}\) and the finetuned model \(\theta_{ft}\) directly in weight space with linear interpolation.
                            Intuitively, the merged model \(\tilde{\theta}\) can <strong> combine generalization capabilities of the pretrained model with the 
                            task-specific adaptation of the finetuned model</strong> to robustly learn a more general version of the target task.

                            where \(\theta_{pre}\) represents the pretrained model parameters,
                            \(\theta_{ft}\) represents the parameters after task-specific finetuning, and \(\alpha\) ∈
                            [0,
                            1] controls the balance between generalization and task specialization.
                        </p>

                        <p><strong>Co-Finetuning</strong></p>
                        <p>
                            We consider two finetuning settings: <strong>task-finetuning (task-FT)</strong>,
                            where we finetune the entire model on a target task dataset \(\mathcal{D}_{\tau}\), and
                            <strong>co-finetuning
                                (co-FT)</strong>, where we finetune on a mix of the target task dataset
                            \(\mathcal{D}_{\tau}\) and the
                            pretraining dataset \(\mathcal{D}_{pre}\).
                            We find that when we have access to the pretraining data, model merging with co-finetuning (RETAIN-co-FT) outperforms model mergine in the task-FT setting (RETAIN-task-FT).
                        </p>

                        <p><strong>Contiual Task Adaptation</strong></p>
                        <p>
                            we can use RETAIN to sequentially add tasks into a pretrained checkpoint by iteratively merging finetuned weights into
the base model and continuing to finetune from the merged checkpoint. This is done by the following formula:
                        </p>

                        <div style="display: flex; justify-content: center; width: 100%; margin: 20px auto;">
                            <img src="images/continual_merging.png" alt="Continual task adaptation" style="width: 60%;">
                        </div>

                    </div>
                </div>
            </div>
        </div>
    </section>

    <section class="section">
        <div class="container is-max-desktop">
            <h2 class="title is-3">Results</h2>

            <!-- <div class="content has-text-justified">
                <p>
                    RETAIN achieves <b>state-of-the-art performance</b> on robotic manipulation tasks when it comes to
                    maintaining strong generalization to out-of-distribution scenarios, as well as retaining generalist
                    capabilities on tasks from pretraining. Our results demonstrate that
                    parameter merging provides a simple yet effective approach to robust finetuning, significantly
                    outperforming standard task finetuning methods that suffer from overfitting. Across two DROID real
                    robot tasks and three LIBERO simulation tasks,
                    <b>RETAIN consistently achieves superior out-of-distribution performance</b> compared to
                    baseline methods.
                </p>
            </div> -->

            <div style="background-color: #e8f4f8; padding: 30px; border-radius: 10px; margin-bottom: 25px;">
                <div class="content has-text-justified">
                    <h3>RETAIN Performance on DROID</h3>
                    <p>
                        On two DROID real robot tasks, RETAIN consistently achieves superior out-of-distribution and generalistperformance compared to the baseline methods, while being comparable in in-distribution performance.
                    </p>
                </div>
            </div>
            <div style="display: flex; justify-content: center; width: 100%; margin: 30px auto 30px auto;">
                <img src="images/avg_droid_results.png" alt="Main DROID results comparison" style="width: 100%;">
            </div>

            <div class="columns">
                <div class="column">

                    <!-- Question 1: Does RETAIN scale with pretraining data? -->
                    <div style="background-color: #e8f4f8; padding: 30px; border-radius: 10px; margin-bottom: 25px;">
                        <div class="content has-text-justified">
                            <h3>RETAIN Scales with Pretraining Data</h3>
                            <p>
                                RETAIN helps OOD generalization much more when the pretrained model is trained on more data,
                                and therefore has more generalist capabilities for ``merging''.
                            </p>
                        </div>
                    </div>
                    <div
                        style="display: flex; justify-content: center; width: 90%; margin: 0 auto; margin-bottom: 60px;">
                        <img src="images/data_scaling.png" alt="RETAIN scaling with pretraining data"
                            style="width: 100%;">
                    </div>

                    <!-- Question 2: Can RETAIN be applied sequentially? -->
                    <div style="background-color: #f0f8e8; padding: 30px; border-radius: 10px; margin-bottom: 25px;">
                        <div class="content has-text-justified">
                            <h3>RETAIN Enables Continual Learning of Tasks</h3>

                            <!-- <p>
                                We extend the model merging above to enable continual task adaptation. This is done by
                                sequentially
                                merging the pretrained model with the finetuned model after each task.
                                $$
                                \tilde{\theta}_{n} = (1 - \alpha) \cdot \tilde{\theta}_{n-1} + \alpha \cdot \theta_{ft,
                                n}
                                $$
                                where \(\theta_{ft, n}\) represents the parameters
                                after finetuning the merged model \(\tilde{\theta}_{n-1}\) on the \(n\)-th task.
                            </p> -->
                            <!-- <img src="images/continual_merging.png" alt="Continual task adaptation"
                                style="width: 100%;"> -->

                            <p>
                                We sequentially apply RETAIN to learn the DROID plates and whiteboard tasks in sequence.
                                Compared to the strongest baseline of sequential co-finetuning on both tasks' datasets,
                                sequential RETAIN is able to maintain its robust generalization to out-of-distribution
                                scenarios for both tasks.
                            </p>
                        </div>
                    </div>
                    <div
                        style="display: flex; justify-content: center; width: 90%; margin: 0 auto; margin-bottom: 60px;">
                        <img src="images/droid_continual_learning.png" alt="Sequential application of RETAIN"
                            style="width: 100%;">
                    </div>

                    <!-- Question 3: What VLA modules matter most for merging? -->
                    <div style="background-color: #fef3e8; padding: 30px; border-radius: 10px; margin-bottom: 25px;">
                        <div class="content has-text-justified">
                            <h3>Language Parameters Matter Most for Merging</h3>
                            <p>
                                Vision-Language-Action models consist of a vision encoder (v), language model backbone (l), and
                                often an action expert (a). We study the effect of merging different parameter groups on performance:
                            </p>

                            <p> $$
                                \begin{pmatrix}
                                \tilde{\theta}_v \\
                                \tilde{\theta}_l \\
                                \tilde{\theta}_a
                                \end{pmatrix}
                                = \left[1-\begin{pmatrix}
                                \alpha_v \\
                                \alpha_l \\
                                \alpha_a
                                \end{pmatrix}\right] \cdot \begin{pmatrix}
                                \theta_{pre,v} \\
                                \theta_{pre,l} \\
                                \theta_{pre,a}
                                \end{pmatrix} + \begin{pmatrix}
                                \alpha_v \\
                                \alpha_l \\
                                \alpha_a
                                \end{pmatrix} \cdot \begin{pmatrix}
                                \theta_{ft,v} \\
                                \theta_{ft,l} \\
                                \theta_{ft,a}
                                \end{pmatrix}
                                $$</p>

                            <p> After conducting a grid search over the coefficients \(\alpha_v\), \(\alpha_l\), and
                                \(\alpha_a\) in simulation, we find that the largest variation in OOD performance is
                                achieved when
                                \(\alpha_l\) is varied, as seen in the color gradient of the plot cube below.

                                We also find that the best performance for a given \(\alpha_l\) is achieved by setting
                                \(\alpha_v = \alpha_a = 1\), as seen in the second plot below.
                            </p>

                            <p> These results suggest that during model merging, it may suffice to only merge the
                                parameters of the language model backbone (\(\alpha_l < 1\), \(\alpha_v = \alpha_a = 1\)). To validate this
                                    hypothesis, we compare the OOD performance of merging all parameters with RETAIN to
                                    only merging language model parameters on 3 LIBERO tasks. The results are shown in
                                    the bar chart below.</p>
                        </div>
                    </div>

                    <!-- First row: 2 images -->
                    <div
                        style="display: flex; justify-content: center; gap: 2%; width: 90%; margin: 0 auto; margin-bottom: 20px;">
                        <img src="images/modality_wise_merging.png" style="width: 49%;" alt="Vision encoder merging">
                        <img src="images/modality_average_over_L_mugs.png" style="width: 49%;"
                            alt="Language encoder merging">
                    </div>
                    <!-- Second row: 1 image -->
                    <div
                        style="display: flex; justify-content: center; width: 90%; margin: 0 auto; margin-bottom: 60px;">
                        <img src="images/ood_multimodal_merging.png" style="width: 60%;" alt="Action decoder merging">
                    </div>

                    <!-- Question 4: How much does the alpha parameter matter?
                    <div style="background-color: #f4e8f8; padding: 30px; border-radius: 10px; margin-bottom: 25px;">
                        <div class="content has-text-justified">
                            <h3>How much does the alpha parameter matter?</h3>
                            <p>
                                The merging coefficient α controls the trade-off between retaining pretrained knowledge
                                and incorporating task-specific adaptations. We perform an ablation study varying
                                \(\alpha\) from
                                0 (pure finetuned model) to 1 (pure pretrained model) and evaluating on the 3 LIBERO
                                tasks. Lining up with real-world results, our results show that model merging helps
                                improve OOD performance as long as the merged model is not too deviated from the
                                finetuned model.
                            </p>
                            <p>
                                While we find in our real-world experiments that RETAIN is robust to different values of
                                \(\alpha\), slight tuning of it is still needed to achieve the best possible OOD
                                performance.
                                An exciting future dierction is determining a good heuristic for choosing the optimal
                                \(\alpha\) value for a given task and dataset.
                            </p>
                        </div>
                    </div>
                    <div
                        style="display: flex; justify-content: center; width: 90%; margin: 0 auto; margin-bottom: 60px;">
                        <img src="images/ood_type_and_alpha_ablation.png" alt="Alpha parameter sensitivity analysis"
                            style="width: 100%;">
                    </div> -->

                </div>
            </div>
        </div>
    </section>

    <section class="section" id="BibTeX">
        <div class="container is-max-desktop">
            <h2 class="title">BibTeX</h2>
            <pre>
      <code>@article{yadav2025retain,
        author = {Yajat Yadav and Zhiyuan Zhou and Andrew Wagenmaker and Karl Pertsch and Sergey Levine},
        title  = {Robust Finetuning of Vision-Language-Action Robot Policies via Parameter Merging},
        conference = {arXiv Pre-print},
        year = {2025},
        url = {https://arxiv.org/abs/2512.08333},
      }
      </code>
    </pre>
        </div>
    </section>
    <br><br>

    <footer class="footer">
        <div class="container is-max-desktop">
            <div class="columns is-centered">
                <div class="column is-8">
                    <div class="content">
                        <p>
                            Website template borrowed from <a
                                href="https://github.com/nerfies/nerfies.github.io">NeRFies</a>
                            under a
                            <a rel="license" href="http://creativecommons.org/licenses/by-sa/4.0/">Creative
                                Commons Attribution-ShareAlike 4.0 International License</a>.
                        </p>
                    </div>
                </div>
            </div>
        </div>
    </footer>

</body>

</html>